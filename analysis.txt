My thoughts:

- The Validation and correction process is completely flawed. The tools are not being used properly, and documentation is not being gathered at all.
- The instructions are too verbose and agents are forgetting them, leading to incorrect tool use.
- I must implement a custom tool that provides info about the mcp tool (mainly query knowledge graph) to be utilised before actually using the mcp tool. -> this might prevent “forgetting”
- Another major concern: the corrector agent NEVER receives the design plan, the selected components and pin details and any calculations or constraints. -> I must fix this so that the corrector does not make uninformed decisions. 
- I must redesign the pipeline to prevent the infinite looping between validator and corrector. 
- I must also rethink the logic for when to use ERC and how to correct ERC issues. -> maybe implement a separate ERC issue resolver agent.

Current state:

# Circuitron Codebase Review: Validation & Correction Loop Analysis

## Executive Summary

The MCP server architecture is correct (single server with multiple aliases), but there are still critical problems with prompt complexity, validation/correction logic, ERC integration, and missing design context.

## Corrected Understanding

### ✅ **MCP Server Architecture (CORRECT)**
- Single MCP server instance serves both documentation and validation needs
- `get_doc_server()` and `get_validation_server()` return the same server with full toolset
- Agents DO have access to both `perform_rag_query` and `query_knowledge_graph`
- **Previous assessment was WRONG** - tool access is not the primary issue

### ❌ **Actual Critical Issues Identified**

## Issue 1: **Prompt Complexity Causing Tool Misuse**

**Location**: prompts.py - CODE_VALIDATION_PROMPT (lines 408-627) and CODE_CORRECTION_PROMPT (lines 629-869)

**Problem**: Extremely verbose prompts (600+ lines) with complex knowledge graph query examples are causing:
- **Token limit pressure** leading to instruction "forgetting"  
- **Over-complex query construction** instead of simple commands
- **False validation failures** due to incorrect tool usage patterns

**Evidence from Prompts**:
```python
# COMPLEX examples that confuse agents:
query_knowledge_graph("query MATCH (c:Class)-[:HAS_METHOD]->(m:Method) WHERE m.name = 'connect' RETURN c.name, m.name, m.params_list LIMIT 10")
query_knowledge_graph("query MATCH (f:Function) WHERE f.name CONTAINS 'generate' RETURN f.name, f.params_list")

# vs SIMPLE commands that should be used first:
query_knowledge_graph("method generate_schematic")
query_knowledge_graph("class Part")
```

## Issue 2: **Missing Design Context in Corrections**

**Location**: utils.py lines 469-498 and pipeline.py lines 148-160

**Problem CONFIRMED**: The correction agent receives ONLY:
- Script content
- Validation issues  
- ERC results

**Missing Critical Context**:
- ❌ Original design plan (functional blocks, rationale, requirements)
- ❌ Selected components and pin details
- ❌ Documentation gathered for the design
- ❌ Design calculations and constraints

**This forces the corrector to make uninformed fixes without understanding design intent.**

## Issue 3: **ERC Integration Completely Broken**

**Location**: Analysis of correction workflow

**Critical Finding**: The corrector agent prompt instructs:
1. "Fix validation issues first using knowledge graph"
2. "THEN run ERC once validation passes"  
3. "Fix ERC issues with targeted corrections"

**But the actual pipeline**:
- Runs ERC automatically in `run_code_validation()` whenever validation passes
- Feeds ERC results to corrector along with validation issues
- **Never calls the `run_erc_tool` directly** - corrector has the tool but doesn't use it
- Creates confusion about when/how to address ERC vs validation issues

## Issue 4: **Flawed Validation/Correction Loop Logic**

**Location**: pipeline.py lines 226-233

**Problems**:
- **Dual condition confusion**: Loop triggers on EITHER validation failure OR ERC failure
- **No separation of concerns**: Validation and ERC issues mixed in single correction cycle
- **No progressive refinement**: Each iteration gets the same context regardless of previous attempts
- **Overly simplistic convergence detection**: Only attempts counter, no issue tracking

## Proposed Solutions

### Solution 1: **Custom Knowledge Graph Context Tool**

Create a lightweight tool that provides curated knowledge graph usage guidance:

```python
@tool("get_kg_usage_guide")
def get_kg_usage_guide(task_type: str) -> str:
    """Get concise knowledge graph query examples for specific validation tasks.
    
    Args:
        task_type: One of 'class_validation', 'method_validation', 'function_validation', 'import_validation'
    """
    guides = {
        "class_validation": """
For validating class instantiation like Part(), Net(), Circuit():
- query_knowledge_graph("class ClassName")
Example: query_knowledge_graph("class Part")
""",
        "method_validation": """
For validating method calls like obj.method_name():
- query_knowledge_graph("method method_name ClassName")  
- query_knowledge_graph("method method_name")  # if class unknown
Example: query_knowledge_graph("method generate_schematic Circuit")
""",
        "function_validation": """
For validating function calls like generate_netlist():
- query_knowledge_graph("query MATCH (f:Function) WHERE f.name = 'function_name' RETURN f")
Example: query_knowledge_graph("query MATCH (f:Function) WHERE f.name = 'generate_netlist' RETURN f")
""",
        "import_validation": """
For validating imports:
- query_knowledge_graph("class ClassName") for class imports
- query_knowledge_graph("query MATCH (f:Function) WHERE f.name = 'function_name' RETURN f") for function imports
"""
    }
    return guides.get(task_type, "Invalid task_type. Use: class_validation, method_validation, function_validation, import_validation")
```

### Solution 2: **Drastically Simplified Agent Prompts**

**New Validation Prompt Structure** (~200 lines vs current 600+):
```
You are Circuitron-Validator. Validate SKiDL scripts using knowledge graph analysis.

PROCESS:
1. Call get_kg_usage_guide() to understand which queries to use
2. Extract all APIs from script (classes, methods, functions)  
3. Use simple knowledge graph commands to validate each API
4. Report findings with confidence score

KNOWLEDGE GRAPH COMMANDS (keep simple):
- "repos" - see available repositories
- "class ClassName" - validate class exists  
- "method methodName ClassName" - validate method on class
- Custom queries only when simple commands insufficient

Use get_kg_usage_guide() first, then apply the recommended simple patterns.
```

**New Correction Prompt Structure** (~150 lines vs current 600+):
```
You are Circuitron-Corrector. Fix validation and ERC issues systematically.

TOOLS:
- get_kg_usage_guide() - Get concise query guidance
- query_knowledge_graph() - Validate APIs (use guide first)
- perform_rag_query() - Get SKiDL documentation
- run_erc_tool() - Run ERC when needed

PROCESS:
1. If validation issues: Use knowledge graph to find correct APIs
2. If ERC issues: Use documentation for ERC resolution patterns
3. Apply fixes and validate changes
4. Use run_erc_tool() only for electrical rule checking

Always call get_kg_usage_guide() before complex knowledge graph queries.
```

### Solution 3: **Enhanced Correction Context**

Update `format_code_correction_input()` to include design context:

```python
def format_code_correction_input(
    script_content: str,
    validation: CodeValidationOutput,
    erc_result: dict[str, object] | None = None,
    plan: PlanOutput | None = None,          # ADD
    selection: PartSelectionOutput | None = None,  # ADD
    docs: DocumentationOutput | None = None,      # ADD
) -> str:
    """Format input for Code Correction agent with full context."""
    
    parts = [
        "CORRECTION CONTEXT",
        "=" * 40,
        "ORIGINAL DESIGN PLAN:",
        format_plan_summary(plan) if plan else "Not available",
        "",
        "SELECTED COMPONENTS:",
        format_selection_summary(selection) if selection else "Not available", 
        "",
        "SCRIPT TO CORRECT:",
        script_content,
        "",
        f"VALIDATION ISSUES: {validation.summary}",
        # ... rest of formatting
    ]
```

### Solution 4: **Separated ERC Workflow**

Redesign the validation/correction loop to separate concerns:

```python
# New pipeline logic
async def enhanced_validation_correction_cycle(code_out, selection, docs, plan):
    # Phase 1: API/Syntax Validation
    validation = await run_code_validation(code_out, selection, docs)
    correction_attempts = 0
    
    while validation.status == "fail" and correction_attempts < 3:
        # Only fix validation issues, no ERC yet
        code_out = await run_code_correction_validation_only(code_out, validation, plan, selection, docs)
        validation = await run_code_validation(code_out, selection, docs)
        correction_attempts += 1
    
    # Phase 2: ERC Validation (only after syntax validation passes)
    if validation.status == "pass":
        erc_result = await run_erc_via_tool(code_out.complete_skidl_code)  # Direct tool call
        erc_attempts = 0
        
        while not erc_result.get("erc_passed", False) and erc_attempts < 3:
            code_out = await run_code_correction_erc_only(code_out, erc_result, plan, selection, docs)
            erc_result = await run_erc_via_tool(code_out.complete_skidl_code)
            erc_attempts += 1
    
    return code_out, validation, erc_result
```

### Solution 5: **Progressive Context Management**

Track correction history and provide incremental context:

```python
class CorrectionContext:
    def __init__(self):
        self.attempt_history = []
        self.resolved_issues = set()
        self.persistent_issues = set()
    
    def add_attempt(self, issues_before, issues_after, corrections_made):
        # Track what's improving vs what's stuck
        self.attempt_history.append({
            'issues_before': issues_before,
            'issues_after': issues_after, 
            'corrections': corrections_made
        })
        
    def get_context_for_next_attempt(self):
        # Provide focused context based on history
        return {
            'focus_areas': self.persistent_issues,
            'successful_patterns': self.extract_successful_patterns(),
            'avoid_patterns': self.extract_failed_patterns()
        }
```

## Implementation Priority

### **Immediate**
1. ✅ Create `get_kg_usage_guide()` custom tool
2. ✅ Drastically optimise validation and correction prompts
3. ✅ Add design context to correction input formatting

### **Next**
4. ✅ Separate validation and ERC correction workflows
5. ✅ Implement direct ERC tool usage in corrector
6. ✅ Add convergence tracking and issue fingerprinting

### **Last**
7. ✅ Progressive context management system
8. ✅ Correction attempt history and pattern learning
9. ✅ Enhanced error reporting and debugging

## Expected Outcomes

**Before Fixes**:
- ❌ Complex knowledge graph queries confusing agents
- ❌ Token limit issues causing instruction forgetting  
- ❌ Corrections without design context failing
- ❌ ERC never properly integrated into correction flow
- ❌ Infinite loops with no meaningful progress

**After Fixes**:
- ✅ Simple, guided knowledge graph usage
- ✅ Concise prompts staying within token limits
- ✅ Context-aware corrections respecting design intent
- ✅ Proper ERC integration with targeted fixes
- ✅ Bounded loops with measurable progress tracking

The core insight is that **tool access isn't the problem - tool usage guidance is**. The agents have the right tools but are overwhelmed by complex instructions and lack proper context about the original design intent.